{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya allah\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inshallah as ins\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from geopy.distance import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shippingLineId</th>\n",
       "      <th>vesselId</th>\n",
       "      <th>CEU</th>\n",
       "      <th>DWT</th>\n",
       "      <th>GT</th>\n",
       "      <th>NT</th>\n",
       "      <th>vesselType</th>\n",
       "      <th>breadth</th>\n",
       "      <th>depth</th>\n",
       "      <th>draft</th>\n",
       "      <th>enginePower</th>\n",
       "      <th>freshWater</th>\n",
       "      <th>fuel</th>\n",
       "      <th>homePort</th>\n",
       "      <th>length</th>\n",
       "      <th>maxHeight</th>\n",
       "      <th>maxSpeed</th>\n",
       "      <th>maxWidth</th>\n",
       "      <th>rampCapacity</th>\n",
       "      <th>yearBuilt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61a8e672f9cba188601e84ab</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8b</td>\n",
       "      <td>6500</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>58684</td>\n",
       "      <td>17606.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>22.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OSLO</td>\n",
       "      <td>199.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>15.2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61ec94f1a8cafc0e93f0e92a</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8d</td>\n",
       "      <td>4902</td>\n",
       "      <td>12325.0</td>\n",
       "      <td>46800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MONROVIA</td>\n",
       "      <td>182.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61e213d5d612676a0f0fb755</td>\n",
       "      <td>61e9f38eb937134a3c4bfd8f</td>\n",
       "      <td>5000</td>\n",
       "      <td>13059.0</td>\n",
       "      <td>46800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAINT JOHN'S</td>\n",
       "      <td>182.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61be24574ea00ae59d0fe388</td>\n",
       "      <td>61e9f38eb937134a3c4bfd91</td>\n",
       "      <td>4200</td>\n",
       "      <td>12588.0</td>\n",
       "      <td>39362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11060.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61a8e673f9cba188601e84ae</td>\n",
       "      <td>61e9f390b937134a3c4bfd93</td>\n",
       "      <td>7450</td>\n",
       "      <td>21052.0</td>\n",
       "      <td>75528</td>\n",
       "      <td>24391.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>22.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>491.47</td>\n",
       "      <td>3236.78</td>\n",
       "      <td>Panama</td>\n",
       "      <td>199.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             shippingLineId                  vesselId   CEU      DWT     GT  \\\n",
       "0  61a8e672f9cba188601e84ab  61e9f38eb937134a3c4bfd8b  6500  21200.0  58684   \n",
       "1  61ec94f1a8cafc0e93f0e92a  61e9f38eb937134a3c4bfd8d  4902  12325.0  46800   \n",
       "2  61e213d5d612676a0f0fb755  61e9f38eb937134a3c4bfd8f  5000  13059.0  46800   \n",
       "3  61be24574ea00ae59d0fe388  61e9f38eb937134a3c4bfd91  4200  12588.0  39362   \n",
       "4  61a8e673f9cba188601e84ae  61e9f390b937134a3c4bfd93  7450  21052.0  75528   \n",
       "\n",
       "        NT  vesselType  breadth  depth  draft  enginePower  freshWater  \\\n",
       "0  17606.0        83.0     32.0  22.20    NaN          0.0         NaN   \n",
       "1      NaN        83.0     31.0    NaN    NaN      14220.0         NaN   \n",
       "2      NaN        83.0     31.0    NaN    NaN      14220.0         NaN   \n",
       "3      NaN        83.0     28.0    NaN    NaN      11060.0         NaN   \n",
       "4  24391.0        83.0     37.2  22.23    NaN      13140.0      491.47   \n",
       "\n",
       "      fuel      homePort  length  maxHeight  maxSpeed  maxWidth  rampCapacity  \\\n",
       "0      NaN          OSLO  199.00        5.0      18.6      15.2         150.0   \n",
       "1      NaN      MONROVIA  182.00        NaN       NaN       NaN           NaN   \n",
       "2      NaN  SAINT JOHN'S  182.00        NaN       NaN       NaN           NaN   \n",
       "3      NaN           NaN  167.00        NaN       NaN       NaN           NaN   \n",
       "4  3236.78        Panama  199.98        NaN       NaN       NaN           NaN   \n",
       "\n",
       "   yearBuilt  \n",
       "0       2000  \n",
       "1       2006  \n",
       "2       2010  \n",
       "3       2011  \n",
       "4       2018  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../CSV/big_files/ais_train.csv\", sep=\"|\")\n",
    "vessels = pd.read_csv(\"../CSV/vessels.csv\", sep=\"|\")\n",
    "test_og = pd.read_csv(\"../CSV/ais_test.csv\")\n",
    "\n",
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIntervals(data0, FEATURES, n=5):\n",
    "    data = data0.copy()\n",
    "    data.sort_values(\"time\", inplace=True)\n",
    "    data[\"day_time\"] = data[\"time\"].apply(lambda t: pd.Timestamp(str(t).split(\" \")[0]))\n",
    "\n",
    "    data[\"month\"] = data[\"time\"].apply(lambda t: t.month)\n",
    "    data[\"day\"] = data[\"time\"].apply(lambda t: t.day)\n",
    "    data[\"hour\"] = data[\"time\"].apply(lambda t: t.hour)\n",
    "\n",
    "    data[\"time\"] = (data[\"time\"] - pd.Timestamp(\"2024-01-01 00:00:00\")).dt.total_seconds()\n",
    "\n",
    "    intervals, sizes, final = [], [], []\n",
    "    \n",
    "    for i, grp in data.groupby(pd.Grouper(key=\"day_time\", freq=f\"{n}D\")):\n",
    "        intervals.append(grp[FEATURES].values)\n",
    "        sizes.append(len(grp[FEATURES].values))\n",
    "\n",
    "    maxSize = max(sizes)\n",
    "\n",
    "    for i in intervals:\n",
    "        residual = maxSize - len(i)\n",
    "        padded = np.pad(i, ((0, residual), (0,0)), mode=\"constant\", constant_values=0)\n",
    "        final.append(padded)\n",
    "\n",
    "    return np.array(final), sizes\n",
    "\n",
    "def normalize_lat_lon(lat, lon):\n",
    "    \"\"\"\n",
    "    Normalizes extreme latitude and longitude values, ensuring:\n",
    "    - Latitude is constrained between [-90, 90] with appropriate longitude shifts.\n",
    "    - Longitude is wrapped to be within [-180, 180].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle extreme latitude values\n",
    "    while lat > 90 or lat < -90:\n",
    "        if lat > 90:\n",
    "            lat = 180 - lat\n",
    "            lon += 180\n",
    "        elif lat < -90:\n",
    "            lat = -180 - lat\n",
    "            lon += 180\n",
    "    \n",
    "    # Normalize longitude using modulo to bring it within [-180, 180]\n",
    "    lon = ((lon + 180) % 360) - 180\n",
    "    \n",
    "    return lat, lon\n",
    "\n",
    "def reconstruct(sol: pd.DataFrame) -> pd.DataFrame:\n",
    "    sol[\"vesselId\"] = last_int[\"vesselId\"]\n",
    "    sol[\"time\"] = last_int[\"time\"]\n",
    "\n",
    "    not_last_int = cleaned[[\"time\", \"vesselId\", \"latitude\", \"longitude\"]].head(cleaned.shape[0] - y_sizes[-1])\n",
    "    sol[\"latitude\"] = np.nan\n",
    "\n",
    "    mongo = []\n",
    "\n",
    "    for i in sol[\"vesselId\"].unique():\n",
    "        current = sol[sol[\"vesselId\"] == i]\n",
    "        last_lat = not_last_int[not_last_int[\"vesselId\"] == i][\"latitude\"].iloc[-1]\n",
    "        last_lon = not_last_int[not_last_int[\"vesselId\"] == i][\"longitude\"].iloc[-1]\n",
    "\n",
    "        current[\"latitude\"] = current[\"delta_lat\"].cumsum() + last_lat\n",
    "        current[\"longitude\"] = current[\"delta_lon\"].cumsum() + last_lon\n",
    "\n",
    "        for _,row in current.iterrows():\n",
    "            mongo.append(row.to_dict())\n",
    "\n",
    "    truth = pd.DataFrame(mongo)\n",
    "    return truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "vesselData = vessels.copy()\n",
    "imputer = KNNImputer(n_neighbors=15)\n",
    "\n",
    "non_numeric = [\"vesselId\", \"shippingLineId\", \"homePort\"]\n",
    "non = vesselData[non_numeric]\n",
    "vesselData.drop(columns=non_numeric, inplace=True)\n",
    "\n",
    "cols, ind = vesselData.columns, vesselData.index\n",
    "\n",
    "vesselData = pd.DataFrame(imputer.fit_transform(vesselData), columns=cols, index=ind)\n",
    "\n",
    "for i in non_numeric:\n",
    "    vesselData[i] = non[i]\n",
    "\n",
    "#Cleaning   \n",
    "speeds = {}\n",
    "for _,row in vesselData.iterrows():\n",
    "    speeds[row[\"vesselId\"]] = row[\"maxSpeed\"]\n",
    "\n",
    "merged = []\n",
    "data[\"maxSpeed\"] = np.nan\n",
    "\n",
    "for i in data[\"vesselId\"].unique():\n",
    "    boat = data[data[\"vesselId\"] == i]\n",
    "    boat[\"maxSpeed\"] = speeds[i]\n",
    "\n",
    "    for _,row in boat.iterrows():\n",
    "        merged.append(row.to_dict())\n",
    "\n",
    "merged = pd.DataFrame(merged, columns=data.columns)\n",
    "merged.sort_values(\"time\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['61e9f3a8b937134a3c4bfdf7' '61e9f3d4b937134a3c4bff1f'\n",
      " '61e9f436b937134a3c4c0131' '61e9f3b4b937134a3c4bfe77'\n",
      " '61e9f41bb937134a3c4c0087' '61e9f468b937134a3c4c028f']\n",
      "['61e9f436b937134a3c4c0131' '61e9f3d4b937134a3c4bff1f'\n",
      " '61e9f3b4b937134a3c4bfe77' '61e9f3a8b937134a3c4bfdf7'\n",
      " '61e9f41bb937134a3c4c0087']\n"
     ]
    }
   ],
   "source": [
    "cleaned = ins.cleanUp(merged, n=5)\n",
    "\n",
    "# cleaned[\"volume\"] = cleaned[\"length\"]*cleaned[\"breadth\"]*cleaned[\"depth\"]\n",
    "\n",
    "cleaned.to_csv(\"cleaned.csv\", sep=\"|\")\n",
    "cleaned.head()\n",
    "\n",
    "print(data[\"vesselId\"].unique()[0:6])\n",
    "print(cleaned[\"vesselId\"].unique()[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "NORMAL = [\"delta_time\", \"speed_from_prev\", \"dist_from_prev\", \"delta_lat_lag_1\", \"delta_lon_lag_1\",\n",
    "          \"delta_lat_cum\", \"delta_lon_cum\", \"delta_lat\", \"delta_lon\"]\n",
    "\n",
    "mean, std = cleaned[NORMAL].mean(), cleaned[NORMAL].std()\n",
    "cleaned[NORMAL] = (cleaned[NORMAL] - mean) / std\n",
    "\n",
    "cleaned[NORMAL].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gen data\n",
    "TARGETS = [\"delta_lat\", \"delta_lon\", \"delta_lat_cum\", \"delta_lon_cum\"]\n",
    "FEATURES = [x for x in NORMAL if x not in TARGETS]\n",
    "\n",
    "X, X_sizes = createIntervals(cleaned, FEATURES)\n",
    "y, y_sizes = createIntervals(cleaned, TARGETS)\n",
    "\n",
    "X_train, X_test = X[:-1], X[-1].reshape((1, X.shape[1], X.shape[2]))\n",
    "y_train, y_test = y[:-1], y[-1]\n",
    "\n",
    "#Parameters\n",
    "sequence_length = X.shape[1]\n",
    "num_features = X.shape[2]\n",
    "num_targets = y.shape[2]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "def lstm():\n",
    "    lstm = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(sequence_length, num_features), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        tf.keras.layers.Dense(num_targets)\n",
    "    ])\n",
    "\n",
    "    lstm.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_pred_lstm = lstm.predict(X_test)\n",
    "\n",
    "    #Loss\n",
    "    y_lstm = y_pred_lstm.reshape(y_test.shape)\n",
    "    print(f\"LSTM MSE: {mean_squared_error(y_test, y_lstm)}\")\n",
    "\n",
    "    return y_lstm\n",
    "\n",
    "#RNN\n",
    "def rnn():\n",
    "    rnn = tf.keras.Sequential([\n",
    "        tf.keras.layers.SimpleRNN(64, activation=\"relu\", input_shape=(sequence_length, num_features), return_sequences=True),\n",
    "        tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",
    "        tf.keras.layers.Dense(num_targets)\n",
    "    ])\n",
    "\n",
    "    rnn.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    rnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_rnn = rnn.predict(X_test).reshape(y_test.shape)\n",
    "    print(f\"RNN MSE: {mean_squared_error(y_test, y_rnn)}\")\n",
    "\n",
    "    return y_rnn\n",
    "\n",
    "#CNN\n",
    "def cnn():\n",
    "    cnn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', padding=\"same\", input_shape=(sequence_length, num_features)),\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=2, activation='relu', padding=\"same\"),\n",
    "        tf.keras.layers.Conv1D(filters=16, kernel_size=2, activation='relu', padding=\"same\"),\n",
    "        tf.keras.layers.Conv1D(filters=2, kernel_size=1) \n",
    "    ])\n",
    "\n",
    "    cnn.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_cnn = cnn.predict(X_test).reshape(y_test.shape)\n",
    "    print(f\"CNN MSE: {mean_squared_error(y_test, y_cnn)}\")\n",
    "\n",
    "    return y_cnn\n",
    "\n",
    "#LSTM with more layers\n",
    "def lstm_plus():\n",
    "    lstm_plus = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(sequence_length, num_features), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64, input_shape=(sequence_length, num_features), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32, input_shape=(sequence_length, num_features), return_sequences=True),\n",
    "        tf.keras.layers.Dense(50),\n",
    "        tf.keras.layers.Dense(num_targets)\n",
    "    ])\n",
    "\n",
    "    lstm_plus.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    lstm_plus.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_lstm_plus = lstm_plus.predict(X_test).reshape(y_test.shape)\n",
    "    print(f\"LSTM Plus MSE: {mean_squared_error(y_test, y_lstm_plus)}\")\n",
    "\n",
    "    return y_lstm_plus\n",
    "\n",
    "#Bidirectional\n",
    "def bi():\n",
    "    bi = tf.keras.Sequential([\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(sequence_length, num_features)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
    "        tf.keras.layers.Dense(num_targets)\n",
    "    ])\n",
    "\n",
    "    bi.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    bi.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_bi = bi.predict(X_test).reshape(y_test.shape)\n",
    "    print(f\"Bidirectional MSE: {mean_squared_error(y_test, y_bi)}\")\n",
    "\n",
    "    return y_bi\n",
    "\n",
    "#Hybrid\n",
    "def hybrid():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(sequence_length, num_features)),\n",
    "        tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(34, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_targets)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    # Check shapes for consistency\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_features)\n",
    "    print(f\"y_train shape: {y_train.shape}\")  # Expected shape: (num_samples, sequence_length, num_targets)\n",
    "\n",
    "    #Training model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    #Predict\n",
    "    y_hybrid = model.predict(X_test).reshape(y_test.shape)\n",
    "    print(f\"Hybrid MSE: {mean_squared_error(y_test, y_hybrid)}\")\n",
    "\n",
    "    return y_hybrid, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 0.7\n",
    "# y_ensemble = x*y_cnn + (1-x)*y_hybrid\n",
    "# print(f\"Ensemble ({x}*CNN and {(1-x)}*LSTM) MSE: {mean_squared_error(y_test, y_ensemble)}\")\n",
    "\n",
    "y_hybrid, model = hybrid()\n",
    "model.save(\"hybrid.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_int = cleaned[[\"time\", \"vesselId\", \"latitude\", \"longitude\" ,\"delta_lat\", \"delta_lon\"]].tail(y_sizes[-1]).reset_index(drop=True)\n",
    "prepred = pd.DataFrame(y_hybrid[0:y_sizes[-1]], columns=[\"delta_lat\", \"delta_lon\", \"delta_lat_cum\", \"delta_lon_cum\"])\n",
    "pretruth = pd.DataFrame(y_test[0:y_sizes[-1]], columns=[\"delta_lat\", \"delta_lon\", \"delta_lat_cum\", \"delta_lon_cum\"])\n",
    "\n",
    "#Un normalize\n",
    "prepred[\"delta_lat\"] = (prepred[\"delta_lat\"] * std[-1]) + mean[-1]\n",
    "prepred[\"delta_lon\"] = (prepred[\"delta_lon\"] * std[-2]) + mean[-2]\n",
    "\n",
    "pretruth[\"delta_lat\"] = (pretruth[\"delta_lat\"] * std[-1]) + mean[-1]\n",
    "pretruth[\"delta_lon\"] = (pretruth[\"delta_lon\"] * std[-2]) + mean[-2]\n",
    "\n",
    "#Reconstruction\n",
    "truth = reconstruct(pretruth)\n",
    "pred = reconstruct(prepred)\n",
    "\n",
    "print(truth[\"vesselId\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel = \"61e9f41bb937134a3c4c0087\"\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(truth[truth[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(cleaned[cleaned[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(pred[pred[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel = \"61e9f41bb937134a3c4c0087\"\n",
    "cleaned = pd.read_csv(\"cleaned.csv\", sep=\"|\")\n",
    "\n",
    "#Iterative style - Start Input: last row of train data.\n",
    "#MSE lat/lon: 62\n",
    "brur = cleaned.tail(y_sizes[-1]).copy()\n",
    "brur = brur[brur[\"vesselId\"] == vessel].reset_index(drop=True)\n",
    "not_last_int = cleaned.head(cleaned.shape[0] - y_sizes[-1]).copy()\n",
    "\n",
    "means, std = not_last_int[NORMAL].copy().mean(), not_last_int[NORMAL].copy().std()\n",
    "not_last_int[NORMAL] = (not_last_int[NORMAL] - means) / std\n",
    "\n",
    "#Delta Time\n",
    "last_time = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][\"time\"]\n",
    "first_delta_time = (brur.at[0, \"time\"] - last_time).total_seconds()\n",
    "\n",
    "brur[\"time\"] = (brur[\"time\"] - pd.to_datetime(\"2024-01-01 00:00:00\")).dt.total_seconds()\n",
    "brur[\"delta_time\"] = brur[\"time\"] - brur[\"time\"].shift(1)\n",
    "brur.at[0, \"delta_time\"] = first_delta_time\n",
    "\n",
    "delta_time = brur[\"delta_time\"].to_numpy()\n",
    "\n",
    "#Start predicting on last row\n",
    "predictions, pos, inputs = [], [], []\n",
    "\n",
    "next_input = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][FEATURES].to_numpy().reshape((1,num_features))\n",
    "prev_pos = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][[\"latitude\", \"longitude\"]].to_list()\n",
    "\n",
    "rows = X_test.shape[1] - 1\n",
    "padded = np.pad(next_input, pad_width=((0, rows), (0, 0)), mode='constant', constant_values=0).reshape(X_test.shape)\n",
    "padded = np.asarray(padded).astype('float32')\n",
    "\n",
    "next_pred = model.predict(padded)[0][0][0:2]\n",
    "un_normalized = [(next_pred[0] * std[-2]) + means[-2], (next_pred[1] * std[-1]) + means[-1]]\n",
    "predictions.append(np.array(un_normalized))\n",
    "\n",
    "new_pos = np.add(prev_pos, un_normalized)\n",
    "pos.append(new_pos)\n",
    "\n",
    "#Speed, Dist and Lags\n",
    "dist = distance((prev_pos[0], prev_pos[1]), (new_pos[0], new_pos[1])).km\n",
    "speed = dist*1000 / brur.iloc[0][\"delta_time\"]\n",
    "lat_lag, lon_lag = next_input[0][-2], next_input[0][-1]\n",
    "\n",
    "next_input = np.array([brur.iloc[0][\"delta_time\"], (speed - means[1]) - std[1], (dist - means[2]) - std[2], lat_lag, lon_lag]).reshape((1, num_features))\n",
    "inputs.append(next_input)\n",
    "\n",
    "for i in range(delta_time.shape[0]-1):\n",
    "    rows = X_test.shape[1] - len(inputs)\n",
    "    current = np.array(inputs[0:i+1]).reshape((i+1, num_features))\n",
    "\n",
    "    padded = np.pad(current, pad_width=((0, rows), (0, 0)), mode='constant', constant_values=0).reshape(X_test.shape)\n",
    "    padded = np.asarray(padded).astype('float32')\n",
    "\n",
    "    #Next prediction\n",
    "    next_pred = model.predict(padded)[0][i+1][0:2]\n",
    "    un_normalized = [(next_pred[0] * std[-2]) + means[-2], (next_pred[1] * std[-1]) + means[-1]]\n",
    "    predictions.append(np.array(next_pred))\n",
    "\n",
    "    prev_pos = pos[i]\n",
    "    new_pos = np.add(prev_pos, un_normalized)\n",
    "    pos.append(new_pos)\n",
    "\n",
    "    #Dist, speed and lags\n",
    "    dist = distance((prev_pos[0], prev_pos[1]), (new_pos[0], new_pos[1])).km\n",
    "    speed = dist*1000 / brur.iloc[i][\"delta_time\"]\n",
    "    lat_lag, lon_lag = predictions[-2][0], predictions[-2][1]\n",
    "\n",
    "\n",
    "    next_input = np.array([(delta_time[i+1] - means[0]) / std[0], (speed - means[1]) - std[1], (dist - means[2]) - std[2], lat_lag, lon_lag]).reshape((1, num_features))\n",
    "    inputs.append(next_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = np.array(predictions)\n",
    "df = pd.DataFrame(pred_arr, columns=[\"delta_lat\", \"delta_lon\"])\n",
    "\n",
    "df[\"vesselId\"] = vessel\n",
    "df[\"time\"] = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel].reset_index(drop=True)[\"time\"]\n",
    "\n",
    "last_pos = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][[\"latitude\", \"longitude\"]].to_list()\n",
    "df[\"latitude\"] = last_pos[0] + df[\"delta_lat\"].cumsum()\n",
    "df[\"longitude\"] = last_pos[1] + df[\"delta_lon\"].cumsum()\n",
    "\n",
    "y_og = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel][[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "y_og.head()\n",
    "y_pred = df[[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "print(f\"MSE: {mean_squared_error(y_og, y_pred)}\")\n",
    "df[[\"vesselId\", \"time\", \"latitude\", \"longitude\"]].to_csv(\"bigga_nalls.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    ins.visualize_vessel_movements(cleaned[cleaned[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(df[df[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel = \"61e9f41bb937134a3c4c0087\"\n",
    "\n",
    "#Input: last block of train data\n",
    "#MSE 8.8\n",
    "brur = cleaned.tail(y_sizes[-1]).copy()\n",
    "brur = brur[brur[\"vesselId\"] == vessel].reset_index(drop=True)\n",
    "not_last_int = cleaned.head(cleaned.shape[0] - y_sizes[-1]).copy()\n",
    "\n",
    "means, std = cleaned[NORMAL].copy().mean(), cleaned[NORMAL].copy().std()\n",
    "not_last_int[NORMAL] = (not_last_int[NORMAL] - means) / std\n",
    "\n",
    "#Delta Time\n",
    "last_time = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][\"time\"]\n",
    "first_delta_time = (brur.at[0, \"time\"] - last_time).total_seconds()\n",
    "\n",
    "brur[\"time\"] = (brur[\"time\"] - pd.to_datetime(\"2024-01-01 00:00:00\")).dt.total_seconds()\n",
    "brur[\"delta_time\"] = brur[\"time\"] - brur[\"time\"].shift(1)\n",
    "brur.at[0, \"delta_time\"] = first_delta_time\n",
    "\n",
    "delta_time = brur[\"delta_time\"].to_numpy()\n",
    "\n",
    "brur[[\"vesselId\", \"delta_time\"]].head()\n",
    "\n",
    "# --- Predicting ---\n",
    "# Setup\n",
    "inputs, predictions, pos = [], [], []\n",
    "no_rows = X_train[0].shape[0]\n",
    "\n",
    "X_vessel, _ = createIntervals(cleaned, FEATURES + [\"vesselId\"])\n",
    "X_vessel_last_df = pd.DataFrame(X_vessel[-2], columns=(FEATURES + [\"vesselId\"]))\n",
    "vessel_df = X_vessel_last_df[X_vessel_last_df[\"vesselId\"] == vessel].copy()\n",
    "vessel_df[FEATURES] = (vessel_df[FEATURES] - means[0:5]) / std[0:5]\n",
    "vessel_arr = vessel_df[FEATURES].to_numpy()\n",
    "\n",
    "\n",
    "#Prediction time\n",
    "i = 0\n",
    "while i <= 0:\n",
    "    #Add new predictions to the window\n",
    "    if len(inputs) != 0:\n",
    "        current = np.array(inputs).reshape((len(inputs), num_features))\n",
    "        next_input = np.concatenate((vessel_arr, current), axis=0)\n",
    "    else:\n",
    "        next_input = vessel_arr\n",
    "\n",
    "    #Pad\n",
    "    remaining = no_rows - next_input.shape[0]\n",
    "    next_input = [np.pad(next_input, pad_width=((0, remaining), (0,0)), mode=\"constant\", constant_values=0)]\n",
    "    next_input = np.asarray(next_input).astype(\"float32\")\n",
    "\n",
    "    #Predict\n",
    "    next_pred = model.predict(next_input)\n",
    "\n",
    "    inputs.append([i for j in range(num_features)])\n",
    "    i += 1\n",
    "\n",
    "#Hold on\n",
    "next_pred = next_pred.reshape((next_pred.shape[1], next_pred.shape[2]))\n",
    "next_pred = next_pred[0:delta_time.shape[0]]\n",
    "\n",
    "df = pd.DataFrame(next_pred, columns=[\"delta_lat\", \"delta_lon\", \"shit1\", \"shit2\"])\n",
    "df.drop(columns=[\"shit1\", \"shit2\"], inplace=True)\n",
    "df[\"delta_lat\"] = (df[\"delta_lat\"] * std[-2]) + means[-2]\n",
    "df[\"delta_lon\"] = (df[\"delta_lon\"] * std[-1]) + means[-1]\n",
    "\n",
    "df[\"vesselId\"] = vessel\n",
    "df[\"time\"] = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel].reset_index(drop=True)[\"time\"]\n",
    "\n",
    "last_pos = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][[\"latitude\", \"longitude\"]].to_list()\n",
    "df[\"latitude\"] = last_pos[0] + df[\"delta_lat\"].cumsum()\n",
    "df[\"longitude\"] = last_pos[1] + df[\"delta_lon\"].cumsum()\n",
    "\n",
    "y_og = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel][[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "y_og.head()\n",
    "y_pred = df[[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "print(f\"MSE: {mean_squared_error(y_og, y_pred)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    ins.visualize_vessel_movements(cleaned[cleaned[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(df[df[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel = \"61e9f41bb937134a3c4c0087\"\n",
    "\n",
    "#Iterative - Start Input: last block of train data\n",
    "#\n",
    "brur = cleaned.tail(y_sizes[-1]).copy()\n",
    "brur = brur[brur[\"vesselId\"] == vessel].reset_index(drop=True)\n",
    "not_last_int = cleaned.head(cleaned.shape[0] - y_sizes[-1]).copy()\n",
    "\n",
    "means, std = cleaned[NORMAL].copy().mean(), cleaned[NORMAL].copy().std()\n",
    "not_last_int[NORMAL] = (not_last_int[NORMAL] - means) / std\n",
    "\n",
    "#Delta Time\n",
    "last_time = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][\"time\"]\n",
    "first_delta_time = (brur.at[0, \"time\"] - last_time).total_seconds()\n",
    "\n",
    "brur[\"time\"] = (brur[\"time\"] - pd.to_datetime(\"2024-01-01 00:00:00\")).dt.total_seconds()\n",
    "brur[\"delta_time\"] = brur[\"time\"] - brur[\"time\"].shift(1)\n",
    "brur.at[0, \"delta_time\"] = first_delta_time\n",
    "\n",
    "delta_time = brur[\"delta_time\"].to_numpy()\n",
    "\n",
    "brur[[\"vesselId\", \"delta_time\"]].head()\n",
    "\n",
    "# --- Predicting ---\n",
    "# Setup\n",
    "inputs, predictions, pos = [], [], []\n",
    "no_rows = X_train[0].shape[0]\n",
    "\n",
    "X_vessel, _ = createIntervals(cleaned, FEATURES + [\"vesselId\"])\n",
    "X_vessel_last_df = pd.DataFrame(X_vessel[-2], columns=(FEATURES + [\"vesselId\"]))\n",
    "\n",
    "vessel_df = X_vessel_last_df[X_vessel_last_df[\"vesselId\"] == vessel].copy()\n",
    "vessel_df[FEATURES] = (vessel_df[FEATURES] - means[0:5]) / std[0:5]\n",
    "vessel_arr = vessel_df[FEATURES].to_numpy()\n",
    "\n",
    "init_pos = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][[\"latitude\", \"longitude\"]].to_list()\n",
    "init_lags = not_last_int[not_last_int[\"vesselId\"] == vessel].iloc[-1][[\"delta_lat\", \"delta_lon\"]].to_numpy()\n",
    "\n",
    "pos.append(init_pos)\n",
    "predictions.append(init_lags)\n",
    "\n",
    "#Prediction time\n",
    "i = 0\n",
    "while i < delta_time.shape[0]:\n",
    "    #Add new predictions to the window\n",
    "    if len(inputs) != 0:\n",
    "        current = np.array(inputs).reshape((len(inputs), num_features))\n",
    "        next_input = np.concatenate((vessel_arr, current), axis=0)\n",
    "    else:\n",
    "        next_input = vessel_arr\n",
    "\n",
    "    #Pad\n",
    "    remaining = no_rows - next_input.shape[0]\n",
    "    next_input = [np.pad(next_input, pad_width=((0, remaining), (0,0)), mode=\"constant\", constant_values=0)]\n",
    "    next_input = np.asarray(next_input).astype(\"float32\")\n",
    "\n",
    "    #Predict\n",
    "    next_pred = model.predict(next_input)\n",
    "    relevant = next_pred[0][(vessel_arr.shape[0]) + i][0:2]\n",
    "    predictions.append(np.array(relevant))\n",
    "\n",
    "    #Calculations\n",
    "    un_normalized = [(relevant[0] * std[-2]) + means[-2], (relevant[1] * std[-1]) + means[-1]]\n",
    "\n",
    "    prev_pos = pos[i]\n",
    "    new_pos = np.add(prev_pos, un_normalized)\n",
    "    pos.append(new_pos)\n",
    "\n",
    "    #Dist, speed and lags\n",
    "    dist = distance((prev_pos[0], prev_pos[1]), (new_pos[0], new_pos[1])).km\n",
    "    speed = dist*1000 / delta_time[i]\n",
    "    lat_lag, lon_lag = predictions[-2][0], predictions[-2][1]\n",
    "\n",
    "    next_input_row = np.array([(delta_time[i] - means[0]) / std[0], (speed - means[1]) - std[1], (dist - means[2]) - std[2], lat_lag, lon_lag]).reshape((1, num_features))\n",
    "\n",
    "    inputs.append(next_input_row)\n",
    "    i += 1\n",
    "\n",
    "pos = np.array(pos[1::])\n",
    "print(pos.shape, delta_time.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pos, columns=[\"latitude\", \"longitude\"])\n",
    "\n",
    "df[\"vesselId\"] = vessel\n",
    "df[\"time\"] = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel].reset_index(drop=True)[\"time\"]\n",
    "\n",
    "y_og = cleaned.tail(y_sizes[-1])[cleaned.tail(y_sizes[-1])[\"vesselId\"] == vessel][[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "y_og.head()\n",
    "y_pred = df[[\"latitude\", \"longitude\"]].copy().reset_index(drop=True)\n",
    "print(f\"MSE: {mean_squared_error(y_og, y_pred)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    ins.visualize_vessel_movements(cleaned[cleaned[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig = (\n",
    "    ins.visualize_vessel_movements(df[df[\"vesselId\"] == vessel])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggie = pd.read_csv(\"bigga_nalls.csv\", sep=\"|\")\n",
    "biggie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_og.copy()\n",
    "test[\"time\"] = pd.to_datetime(test[\"time\"])\n",
    "test.drop(columns=[\"scaling_factor\"], inplace=True)\n",
    "\n",
    "cleaned = pd.read_csv(\"cleaned.csv\", sep=\"|\")\n",
    "cleaned[\"time\"] = pd.to_datetime(cleaned[\"time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test[\"vesselId\"].unique()[0:5]:\n",
    "    boat = test[test[\"vesselId\"] == i].reset_index(drop=True)\n",
    "    boat_train = cleaned[cleaned[\"vesselId\"] == i].reset_index(drop=True)\n",
    "\n",
    "    # last_time = boat_train\n",
    "    # boat[\"delta_time\"] = (boat[\"time\"] - boat[\"time\"].shift(1))\n",
    "    # boat.at[0, \"delta_time\"] = boat.at[0, \"time\"] - last_time\n",
    "    # boat[\"delta_time\"] = boat[\"delta_time\"].dt.total_seconds()\n",
    "\n",
    "    print(boat_train.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[\"vesselId\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
